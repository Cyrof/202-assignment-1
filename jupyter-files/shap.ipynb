{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# SHapley Additive exPlanations (SHAP)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Library Imports "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import pickle\n", "\n", "import pandas as pd\n", "import numpy as np\n", "import shap\n", "from matplotlib import pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["RAND_STATE = 0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Importing the Test Sets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset_folder = f\"{os.path.abspath(os.path.join(os.getcwd(), os.pardir))}/datasets\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_test = pd.read_csv(os.path.join(dataset_folder, \"obesity_X_test.csv\"), index_col=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As with the rest of the stages, we make a variant of the sets without the height and weight columns for comparison:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_test_no_hw = X_test.drop([\"Height\", \"Weight\"], axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Importing the Random Forest Classifiers\n", "We import the random forest classifiers that we trained previously. SHAP values will be calculated for these models to explain their predictions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def import_model(filename):\n", "    file_path = f\"{os.path.abspath(os.path.join(os.getcwd(), os.pardir))}/models/{filename}\"\n", "    with open(file_path, 'rb') as file: \n", "        model = pickle.load(file)\n", "    print(f\"Model imported from {file_path}\")\n", "    return model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rand_forest = import_model(\"rand_forest.pkl\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rand_forest_no_hw = import_model(\"rand_forest_no_hw.pkl\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Importing Encoders\n", "We import the label encoder for the target feature so that we can encode the original values for indexing purposes."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def import_encoder(filename):\n", "    file_path = f\"{os.path.abspath(os.path.join(os.getcwd(), os.pardir))}/encoders/{filename}\"\n", "    with open(file_path, 'rb') as file: \n", "        encoder = pickle.load(file)\n", "    print(f\"Encoder imported from {file_path}\")\n", "    return encoder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["target_le = import_encoder(\"target_le.pkl\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["target_class_label_d = {cls: idx for idx, cls in enumerate(target_le.classes_)}\n", "target_class_label_d"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We also import the one-hot encoder and scaler, which are needed when processing the SHAP values:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nominal_ohe = import_encoder(\"nominal_ohe.pkl\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = import_encoder(\"scaler.pkl\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Explanations"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Calculating the SHAP Values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First, we create the explainers and calculate the SHAP values:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explainer = shap.TreeExplainer(rand_forest)\n", "shap_values = explainer(X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explainer_no_hw = shap.TreeExplainer(rand_forest_no_hw)\n", "shap_values_no_hw = explainer_no_hw(X_test_no_hw)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Preprocessing"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Undoing the One-Hot Encoding\n", "However, because our nominal features are one-hot encoded, their SHAP values have been \"separated\". We need to group these one-hot encoded features back together. For reference, here is a list of all column after one-hot encoding:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_test.columns"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### Summing Shape Values\n", "We first need to calculate the SHAP values for each nominal feature, which is done by taking the sum of the SHAP values for each feature's corresponding set of one-hot encoded features. We begin by determining the number of output one-hot encoded features for each nominal feature:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_ohe_feats: dict[str, int] = {\n", "    feat_name: (len(categories) if drop_idx is None else len(categories) - 1)\n", "    for feat_name, categories, drop_idx\n", "    in zip(nominal_ohe.feature_names_in_, nominal_ohe.categories_, nominal_ohe.drop_idx_)\n", "}\n", "n_ohe_feats"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our one-hot encoded dataset has the one-hot encoded features after all numerical features.\n", "Hence, the number of numerical features will tell us the start index of the first one-hot encoded feature:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["numerical_feature_count = 8\n", "numerical_feature_count_no_hw = 6"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We will now define a function to perform the summation for us.\n", "\n", "To calculate the sum, we first group the SHAP value columns by their original nominal features.\n", "Then, we sum the SHAP values for each group column-wise.\n", "After that, we simply concatenate back the SHAP values for the numerical features."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def undo_shap_values_ohe(svals, with_hw=True):\n", "    num_feat_count = numerical_feature_count if with_hw else numerical_feature_count_no_hw\n", "    # Split the SHAP values for the one-hot encoded features.\n", "    # Each entry in split contains the SHAP values for the multiple one-hot encoded features of the original categorical feature.\n", "    values_split = np.split(svals.values[:, num_feat_count:, :], np.cumsum(list(n_ohe_feats.values())[:-1]), axis=1)\n", "    \n", "    # Sum the SHAP values for each group.\n", "    values_summed = np.array([vals.sum(axis=1) for vals in values_split])\n", "    \n", "    # We need to swap the first two axes since the first axis should index the instances and the second axis should index the features.\n", "    unohe_values = np.swapaxes(values_summed, 0, 1)\n", "    \n", "    # Finally, we concatenate back the SHAP values for the numerical features.\n", "    new_values = np.concatenate((svals.values[:, :num_feat_count, :], unohe_values), axis=1)\n", "    \n", "    return new_values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We now apply the function to our two sets of SHAP values:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_values = undo_shap_values_ohe(shap_values)\n", "new_values_no_hw = undo_shap_values_ohe(shap_values_no_hw, with_hw=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As a sanity check, we check the shape. The first axis's value should be the same as the number of instances, the second axis's value should be the number of features before one-hot encoding and the third axis's value should be `2` since we only have two different categories for the target feature."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_values.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_values_no_hw.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["At last, we can replace the old SHAP values:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap_values.values = new_values\n", "shap_values_no_hw.values = new_values_no_hw"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### Fixing the Data Values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Unfortunately, we are not done. We've replaced the SHAP values, but the data values are still one-hot encoded!\n", "We will need to undo the one-hot encoding, which is, thankfully, fairly straightforward since we can reuse our one-hot encoder."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def undo_shap_data_ohe(svals, with_hw=True):\n", "    num_feature_count = numerical_feature_count if with_hw else numerical_feature_count_no_hw\n", "    unohe_data = nominal_ohe.inverse_transform(svals.data[:, num_feature_count:])\n", "    new_data = np.concatenate((svals.data[:, :num_feature_count], unohe_data), axis=1)\n", "    return new_data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_data = undo_shap_data_ohe(shap_values)\n", "new_data_no_hw = undo_shap_data_ohe(shap_values_no_hw, with_hw=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As before, we check the shape as a sanity check:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_data.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_data_no_hw.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, we replace the data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap_values.data = new_data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap_values_no_hw.data = new_data_no_hw"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### Fixing Feature Names\n", "Lastly, we need to update the feature names since the old feature names includes the one-hot encoded features. This is straightforward:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ohe_feat_names = [name if name != \"family_history_with_overweight\" else \"Family History of Overweightness\" for name in n_ohe_feats.keys()]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap_values.feature_names = X_test.columns[:numerical_feature_count].to_list() + ohe_feat_names\n", "shap_values.feature_names"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap_values_no_hw.feature_names = X_test_no_hw.columns[:numerical_feature_count_no_hw].to_list() + ohe_feat_names\n", "shap_values_no_hw.feature_names"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Undoing the Standardisation\n", "To make it easier to interpret the graphs later on, we will undo the standardisation for SHAP values' `.data`. We define a function for performing this task:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["feature_indices = {name: idx for idx, name in enumerate(shap_values.feature_names) if name in scaler.feature_names_in_}\n", "feature_indices_no_hw = {name: idx for idx, name in enumerate(shap_values_no_hw.feature_names) if name in scaler.feature_names_in_}\n", "\n", "height_col = feature_indices[\"Height\"]\n", "weight_col = feature_indices[\"Weight\"]\n", "\n", "def undo_shap_data_scaling(svals, with_hw=True):\n", "    feat_indices = list(feature_indices.values()) if with_hw else list(feature_indices_no_hw.values())\n", "    numerical_data = svals.data[:, feat_indices]\n", "    \n", "    if not with_hw:\n", "        # A little hacky â€” the scaler expects the height and weight columns to be there, but they aren't,\n", "        # so we add fake height and weight columns temporarily, then remove them after scaling.\n", "        fake_col_1 = min(height_col, weight_col)\n", "        fake_col_2 = max(height_col, weight_col)\n", "        \n", "        copy_with_fake = np.zeros((numerical_data.shape[0], numerical_data.shape[1] + 2))\n", "        copy_with_fake[:, :fake_col_1] = numerical_data[:, :fake_col_1]\n", "        copy_with_fake[:, fake_col_1 + 1:fake_col_2] = numerical_data[:, fake_col_1:fake_col_2]\n", "        copy_with_fake[:, fake_col_2 + 1:] = numerical_data[:, fake_col_2 - 1:]\n", "        \n", "        numerical_data = copy_with_fake.astype(\"float64\")\n", "        \n", "    numerical_data = scaler.inverse_transform(numerical_data)\n", "\n", "    # Drop the fake columns.\n", "    if not with_hw:\n", "        numerical_data = np.delete(numerical_data, [height_col, weight_col], axis=1)\n", "\n", "    new_data = svals.data.copy()\n", "    new_data[:, feat_indices] = numerical_data\n", "\n", "    return new_data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, we undo the standardisation for both sets of SHAP values:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap_values.data = undo_shap_data_scaling(shap_values)\n", "shap_values_no_hw.data = undo_shap_data_scaling(shap_values_no_hw, with_hw=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Utility Functions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We define a function to help us plot a boxplot for a categorical feature against the SHAP values:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Adapted from: https://towardsdatascience.com/shap-for-categorical-features-7c63e6a554ea\n", "def boxplot_categories(svals, feature: str, target_class: int, feature_display: str = None, transform_category=lambda x: x):\n", "    values = svals[:, feature, target_class].values\n", "    data = svals[:, feature, target_class].data\n", "    categories = np.unique(data)\n", "    \n", "    groups = []\n", "    for c in categories:\n", "        relevant_values = values[data == c]\n", "        groups.append(relevant_values)\n", "    \n", "    labels = [transform_category(category) for category in categories]\n", "    \n", "    plt.figure(figsize=(8, 5))\n", "    plt.boxplot(groups, tick_labels=labels)\n", "    plt.ylabel('SHAP Values', size=15)\n", "    plt.xlabel(feature_display if feature_display is not None else feature, size=15);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### With Height and Weight\n", "We can now plot graphs for the SHAP values. Since the SHAP values for the non-obese class are just the negation of those of the obese class,\n", "it is sufficient to plot the values for only one of the classes. The following is a beeswarm diagram for the obese class:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap.plots.beeswarm(shap_values[:, :, target_class_label_d[\"Yes\"]], max_display=X_test.shape[1])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scatter plot for `FCVC`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap.plots.scatter(shap_values[:, \"FCVC\", target_class_label_d[\"Yes\"]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scatter plot for `FAF`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap.plots.scatter(shap_values[:, \"FAF\", target_class_label_d[\"Yes\"]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scatter plot for `Age`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap.plots.scatter(shap_values[:, \"Age\", target_class_label_d[\"Yes\"]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A boxplot for the family history of overweightness feature:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boxplot_categories(\n", "    shap_values,\n", "    \"Family History of Overweightness\",\n", "    target_class_label_d[\"Yes\"],\n", "    transform_category=lambda c: c.title()\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A box plot for `CAEC`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boxplot_categories(\n", "    shap_values,\n", "    \"CAEC\",\n", "    target_class_label_d[\"Yes\"],\n", "    transform_category=lambda c: c.title()\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A boxplot for `FAVC`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boxplot_categories(\n", "    shap_values,\n", "    \"FAVC\",\n", "    target_class_label_d[\"Yes\"],\n", "    transform_category=lambda c: c.title()\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Without Height and Weight\n", "Similarly, we plot graphs for the model trained without the height and weight."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap.plots.beeswarm(shap_values_no_hw[:, :, target_class_label_d[\"Yes\"]], max_display=X_test_no_hw.shape[1])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scatter plot for `FCVC`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap.plots.scatter(shap_values_no_hw[:, \"FCVC\", target_class_label_d[\"Yes\"]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scatter plot for `FAF`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap.plots.scatter(shap_values_no_hw[:, \"FAF\", target_class_label_d[\"Yes\"]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scatter plot for `Age`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap.plots.scatter(shap_values_no_hw[:, \"Age\", target_class_label_d[\"Yes\"]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A boxplot for the `family_history_with_overweight` feature:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boxplot_categories(\n", "    shap_values_no_hw,\n", "    \"Family History of Overweightness\",\n", "    target_class_label_d[\"Yes\"],\n", "    transform_category=lambda c: c.title()\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A box plot for `CAEC`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boxplot_categories(\n", "    shap_values_no_hw,\n", "    \"CAEC\",\n", "    target_class_label_d[\"Yes\"],\n", "    transform_category=lambda c: c.title()\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A boxplot for `FAVC`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boxplot_categories(\n", "    shap_values_no_hw,\n", "    \"FAVC\",\n", "    target_class_label_d[\"Yes\"],\n", "    transform_category=lambda c: c.title()\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A box plot for `MTRANS`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boxplot_categories(\n", "    shap_values_no_hw,\n", "    \"MTRANS\",\n", "    target_class_label_d[\"Yes\"],\n", "    transform_category=lambda c: c.title()\n", ")"]}], "metadata": {"language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 4}
