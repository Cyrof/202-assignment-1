{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f201f51-5ade-40c6-8bf4-9b027b05eefb",
   "metadata": {},
   "source": [
    "# Data Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335d12cd-96b4-454a-a84b-110a03ef24d5",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf386b0-e55b-4063-afbd-ea1a8a09da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50415afc-3157-4e09-b8dd-947e0ecd7760",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7e824-7893-4890-841e-3e62029e9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = f\"{os.path.abspath(os.path.join(os.getcwd(), os.pardir))}/datasets/obesityData.csv\"\n",
    "\n",
    "ob_df = pd.read_csv(csv_path) # ob_df --> obesity dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6a5b8-57c2-42e7-8df0-0974f0235906",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a312d4-80be-4626-afbe-864e4344d413",
   "metadata": {},
   "source": [
    "### Splitting the Dataset\n",
    "The dataset is split into a training set and test set with a ratio of 70% for the training set size and 30% for the test set size. The split is stratified according to the obesity level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e3b5e-9dfa-4696-bd4d-5d8a1cfc8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND_STATE = 0\n",
    "\n",
    "X = ob_df.drop(columns=[\"NObeyesdad\"])\n",
    "y = ob_df[\"NObeyesdad\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, stratify=y, random_state=RAND_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818226ed-11d9-433a-ac55-e6736c7c82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9268f75-35a0-4d05-9e04-f93f3d593266",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1834968-a6e7-49db-9048-2cc97c5478f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9775ac-8cb4-4258-b906-233d75ef3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc323c2-3566-404c-8ef1-ae6084e1b28f",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Standardisation is used to scale the numerical features. Note that the scaler is fitted to the train set, not the test set, to prevent \"leaking\" of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278e736-596e-4fe5-9c1e-1c5816387a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\"Age\", \"Height\", \"Weight\", \"FCVC\", \"NCP\", \"CH2O\", \"FAF\", \"TUE\"]\n",
    "\n",
    "scaler = StandardScaler().fit(X_train[numerical_features])\n",
    "\n",
    "X_train[numerical_features] = scaler.transform(X_train[numerical_features])\n",
    "X_test[numerical_features] = scaler.transform(X_test[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a6d38-a058-4cc0-9c55-655adf9da64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42669e1c-bb68-40d6-8729-f99fc24ada22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41531b10-a447-44f0-b6a6-e0ffcfd47737",
   "metadata": {},
   "source": [
    "### Categorical Feature Encoding\n",
    "The categorical features need to be encoded, using either label encoding or one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4187423-a69e-46c3-a329-cb5ff18cf639",
   "metadata": {},
   "source": [
    "#### Target Feature\n",
    "The target feature (obesity level) is encoded using label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866270c-96df-429a-bdcb-bf34028bd4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(ob_df[\"NObeyesdad\"])\n",
    "\n",
    "# Update the y dataframes.\n",
    "y_train.loc[:] = le.transform(y_train)\n",
    "y_test.loc[:] = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11f195-e3af-42a7-b40a-7681993a87e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcde8a8-0cb5-4d4a-8d85-8203da1f69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797f44a-06ef-46ec-84d8-1f434d8c988f",
   "metadata": {},
   "source": [
    "#### Non-Target Features\n",
    "We now encode the other categorical features. In this case, they all happen to be nominal, hence one-hot encoding is used.\n",
    "The first column for each feature after encoding is dropped.\n",
    "First, we create the one-hot encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286adf5-6de0-493e-86cc-41f4dbd32d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_features = [\n",
    "    \"Gender\",\n",
    "    \"family_history_with_overweight\",\n",
    "    \"FAVC\",\n",
    "    \"CAEC\",\n",
    "    \"SMOKE\",\n",
    "    \"SCC\",\n",
    "    \"CALC\",\n",
    "    \"MTRANS\"\n",
    "]\n",
    "\n",
    "ohe = OneHotEncoder(drop=\"first\", sparse_output=False)\n",
    "ohe.fit(ob_df[nominal_features]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e392b92-6977-4e74-aa1b-a7d25f3e297b",
   "metadata": {},
   "source": [
    "Next, we transform the categorical features' values in `X_train` and `X_test` using the encoder.\n",
    "The original `X_train` and `X_test` dataframes are then modified to use the columns from the one-hot encoding. The old non-encoded features are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da032647-4fa9-4cff-88d2-2677098d74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_train_transformed = ohe.transform(X_train[nominal_features])\n",
    "\n",
    "# Convert back into a dataframe.\n",
    "pd_train_transformed = pd.DataFrame(ohe_train_transformed, columns=ohe.get_feature_names_out(), index=X_train.index)\n",
    "\n",
    "X_train = pd.concat([X_train.drop(nominal_features, axis=1), pd_train_transformed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e48cc0-c5ed-4e26-8ebe-783b5e46ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_train_transformed = ohe.transform(X_test[nominal_features])\n",
    "\n",
    "# Convert back into a dataframe.\n",
    "pd_train_transformed = pd.DataFrame(ohe_train_transformed, columns=ohe.get_feature_names_out(), index=X_test.index)\n",
    "\n",
    "X_test = pd.concat([X_test.drop(nominal_features, axis=1), pd_train_transformed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da86d3-fbea-461e-9974-a5ec35a9c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547d6d7-3afd-48e8-acef-3d23393fd2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676104ac-55ed-45f5-86d0-5b54a1981a5f",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "Nested cross-validation with stratified 10-fold cross-validation and grid search is used for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f4786-347c-4739-a750-7abefda2e712",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af10a09f-02ec-41a2-a672-f6839fd7a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import multiprocessing as mp\n",
    "\n",
    "RAND_STATE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5295d0-e55f-4839-92cd-6894eb99d48c",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "Logistic regression, random forests, decision trees and support vector machines are selected for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e0eff-ba2e-4dd0-8ac6-1a1a4292857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RAND_STATE),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RAND_STATE),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RAND_STATE),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=RAND_STATE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea98620-87b6-4bd9-9106-8ec5d2383fba",
   "metadata": {},
   "source": [
    "### Hyperparameter Grids\n",
    "\n",
    "We will be using a grid search, so we define the hyperparameter grids for tuning each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30d3de-87c5-48f5-96e9-d69f416bbce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "    'Random Forest': {'n_estimators': [50, 100, 200, 300, 400], 'max_depth': [None, 5, 10, 20, 30]},\n",
    "    'Decision Tree': {'max_depth': [None, 5, 10, 20, 30], 'min_samples_split': [2, 5, 10, 20]},\n",
    "    'SVM': {'C': [0.01, 0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f6efa-e734-407b-bcb9-eb49494f39d9",
   "metadata": {},
   "source": [
    "### Performing Nested Cross-Validation\n",
    "\n",
    "We perform nested cross-validation using 10 folds (stratified) and a grid search for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefca550-5e78-4104-bb4f-3e94b812d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs nested cross-validation for a model.\n",
    "def cross_validate(model_name, model):\n",
    "    # Use stratified k-fold with k = 10.\n",
    "    inner_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=RAND_STATE)\n",
    "    \n",
    "    # Perform hyperparameter tuning using a random search.\n",
    "    search_cv = GridSearchCV(\n",
    "        model,\n",
    "        param_grids[model_name],\n",
    "        cv=inner_cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-3 # Use 2 less than the number of CPUs\n",
    "    )\n",
    "    search_cv.fit(X_train, y_train.astype(\"int\"))\n",
    "    \n",
    "    return model_name, search_cv\n",
    "\n",
    "for model_name, search_cv in (cross_validate(model_name, model) for model_name, model in models.items()):\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best Score: {search_cv.best_score_:.3f}\")\n",
    "    print(f\"Best Parameters: {search_cv.best_params_}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
