{"cells": [{"cell_type": "markdown", "id": "2f201f51-5ade-40c6-8bf4-9b027b05eefb", "metadata": {}, "source": ["# Local Interpretable Model-Agnostic Explanations (LIME)"]}, {"cell_type": "markdown", "id": "335d12cd-96b4-454a-a84b-110a03ef24d5", "metadata": {}, "source": ["## Library Imports"]}, {"cell_type": "code", "execution_count": null, "id": "5bf386b0-e55b-4063-afbd-ea1a8a09da27", "metadata": {}, "outputs": [], "source": ["import os\n", "import pickle\n", "import warnings\n", "\n", "import pandas as pd\n", "from sklearn.preprocessing import LabelEncoder\n", "import lime.lime_tabular\n", "import lime.submodular_pick\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "id": "ed80ddb3-171c-40b9-a5d9-4a81652c4dbf", "metadata": {}, "outputs": [], "source": ["RAND_STATE = 0"]}, {"cell_type": "markdown", "id": "50415afc-3157-4e09-b8dd-947e0ecd7760", "metadata": {}, "source": ["## Importing the Train and Test Sets\n", "We load the both the unencoded and encoded `X` train and test sets.\n", "\n", "The unencoded versions are required because we will need to pass them to a `lime` explainer later and, unfortunately, `lime` performs its own scaling and does not handle one-hot encoded features. Hence, they need to be re-encoded differently."]}, {"cell_type": "code", "execution_count": null, "id": "03a7e824-7893-4890-841e-3e62029e9517", "metadata": {}, "outputs": [], "source": ["datasets_folder = f\"{os.path.abspath(os.path.join(os.getcwd(), os.pardir))}/datasets\""]}, {"cell_type": "code", "execution_count": null, "id": "1afc8d64-4d3f-4f42-8d80-97f6424525ef", "metadata": {}, "outputs": [], "source": ["X_train_unencoded = pd.read_csv(os.path.join(datasets_folder, \"obesity_X_train_unencoded.csv\"), index_col=0)\n", "X_train_unencoded"]}, {"cell_type": "code", "execution_count": null, "id": "0cf9052b-9a58-4b66-a8b5-c33d8ea3789f", "metadata": {}, "outputs": [], "source": ["X_test_unencoded = pd.read_csv(os.path.join(datasets_folder, \"obesity_X_test_unencoded.csv\"), index_col=0)\n", "X_test_unencoded"]}, {"cell_type": "code", "execution_count": null, "id": "54f861cc-e603-4c93-85d2-221e089ee44b", "metadata": {}, "outputs": [], "source": ["X_train = pd.read_csv(os.path.join(datasets_folder, \"obesity_X_train.csv\"), index_col=0)\n", "X_train"]}, {"cell_type": "code", "execution_count": null, "id": "0827e36c-027b-4d1c-88f2-eb6be36d0684", "metadata": {}, "outputs": [], "source": ["X_test = pd.read_csv(os.path.join(datasets_folder, \"obesity_X_test.csv\"), index_col=0)\n", "X_test"]}, {"cell_type": "markdown", "id": "45f157df-2c92-4b5d-8c82-a724c3161bdd", "metadata": {}, "source": ["We also import `y_test` so that we can choose an instance from each obesity class to explain:"]}, {"cell_type": "code", "execution_count": null, "id": "4bc7314e-9344-41f6-a233-4f5872c1cd15", "metadata": {}, "outputs": [], "source": ["y_test = pd.read_csv(os.path.join(datasets_folder, \"obesity_y_test.csv\"), index_col=0)[\"Obese\"]\n", "y_test"]}, {"cell_type": "markdown", "id": "b608a7d7-c614-43e2-b1c8-48d16c3533ce", "metadata": {}, "source": ["## Importing Encoders"]}, {"cell_type": "markdown", "id": "9b2288f3-06fb-4bbd-8f85-ff27e6ec4282", "metadata": {}, "source": ["We import some of the encoders we will need:"]}, {"cell_type": "code", "execution_count": null, "id": "a5510983-5705-45e1-ac41-cda0acda42c4", "metadata": {}, "outputs": [], "source": ["def import_encoder(filename):\n", "    file_path = f\"{os.path.abspath(os.path.join(os.getcwd(), os.pardir))}/encoders/{filename}\"\n", "    with open(file_path, 'rb') as file: \n", "        encoder = pickle.load(file)\n", "    print(f\"Encoder imported from {file_path}\")\n", "    return encoder"]}, {"cell_type": "code", "execution_count": null, "id": "7818b793-a0d7-4d50-a363-3ddd48a8a90b", "metadata": {}, "outputs": [], "source": ["scaler = import_encoder(\"scaler.pkl\")"]}, {"cell_type": "code", "execution_count": null, "id": "b9a1cf3a-ffb3-4784-8038-1b09f4ac5dd4", "metadata": {}, "outputs": [], "source": ["nominal_ohe = import_encoder(\"nominal_ohe.pkl\")"]}, {"cell_type": "code", "execution_count": null, "id": "6b4a287a-9683-4c92-9d2a-cbe97a3fecdc", "metadata": {}, "outputs": [], "source": ["target_le = import_encoder(\"target_le.pkl\")"]}, {"cell_type": "markdown", "id": "aeed3757-5a4c-412b-822e-fc25ffb78d18", "metadata": {}, "source": ["## Importing the Random Forest Classifiers\n", "We import the random forest classifiers that we trained previously. LIME will be applied to these models to explain their predictions."]}, {"cell_type": "code", "execution_count": null, "id": "98ff17b8-b8ab-4cb4-9837-1753fc5b2750", "metadata": {}, "outputs": [], "source": ["def import_model(filename):\n", "    file_path = f\"{os.path.abspath(os.path.join(os.getcwd(), os.pardir))}/models/{filename}\"\n", "    with open(file_path, 'rb') as file: \n", "        model = pickle.load(file)\n", "    print(f\"Model imported from {file_path}\")\n", "    return model"]}, {"cell_type": "code", "execution_count": null, "id": "acd6b9d3-405b-4041-8692-3f0d20c8207f", "metadata": {}, "outputs": [], "source": ["rand_forest = import_model(\"rand_forest.pkl\")"]}, {"cell_type": "code", "execution_count": null, "id": "2bb573ff-640c-4079-bd68-0f0c1372f19e", "metadata": {}, "outputs": [], "source": ["rand_forest_no_hw = import_model(\"rand_forest_no_hw.pkl\")"]}, {"cell_type": "markdown", "id": "89e6a5b8-57c2-42e7-8df0-0974f0235906", "metadata": {}, "source": ["## Preprocessing"]}, {"cell_type": "markdown", "id": "4f855b9e-231b-4bd2-9b90-bfa76964862b", "metadata": {}, "source": ["`lime` is a little troublesome, unfortunately. When creating an explainer, the train set must be passed to the constructor. However, we must be careful with the encoding of the train set.\n", "\n", "First, all categorical features should be encoded using label encoding. One-hot encoding should not be used â€” instead, one-hot encoding should only be applied through the `predict_fn` argument of `explain_instance()`.\n", "\n", "`lime` also does its own feature scaling. We could pre-scale the data using standardisation like we did before. However, `lime` will apply its own feature scaling on top of that. It also becomes difficult to interpret the explainer's results since they will be with respect to the scaled values.\n", "\n", "Hence, we will need to preprocess the data into a format suitable for consumption by the explainer. This involves encoding all categorical features using label encoding, but _not_ performing feature scaling. Then, we will have to reprocess the data the explainer passes to `predict_fn` into a format that our trained model understands."]}, {"cell_type": "markdown", "id": "772c2e7b-15e3-44ac-bb1d-2b461e7d47ba", "metadata": {}, "source": ["First, we make copies of the unencoded sets:"]}, {"cell_type": "code", "execution_count": null, "id": "297bf4ec-6631-4796-a6c6-4b0292dde654", "metadata": {}, "outputs": [], "source": ["X_train_ex = X_train_unencoded.copy()"]}, {"cell_type": "code", "execution_count": null, "id": "9296ec7a-b75e-4059-8667-b90adf906518", "metadata": {}, "outputs": [], "source": ["X_test_ex = X_test_unencoded.copy()"]}, {"cell_type": "markdown", "id": "8799824d-96c3-4b9a-a089-ed32ff221438", "metadata": {}, "source": ["We encode all categorical features (which all happen to be nominal) using label encoding:"]}, {"cell_type": "code", "execution_count": null, "id": "b60ed529-2174-4c90-898e-8732aad4ccec", "metadata": {}, "outputs": [], "source": ["nominal_features = [\n", "    \"Gender\",\n", "    \"family_history_with_overweight\",\n", "    \"FAVC\",\n", "    \"CAEC\",\n", "    \"SMOKE\",\n", "    \"SCC\",\n", "    \"CALC\",\n", "    \"MTRANS\"\n", "]\n", "\n", "label_encoders = {feature: None for feature in nominal_features}\n", "for feature in label_encoders.keys():\n", "    le = LabelEncoder()\n", "    feature_all = pd.concat((X_train_unencoded[feature], X_test_unencoded[feature]))\n", "    le.fit(feature_all)\n", "    X_train_ex[feature] = le.transform(X_train_ex[feature])\n", "    X_test_ex[feature] = le.transform(X_test_ex[feature])\n", "    label_encoders[feature] = le"]}, {"cell_type": "code", "execution_count": null, "id": "aaad5660-fa85-4636-bf7a-f3f94db095d7", "metadata": {}, "outputs": [], "source": ["X_train_ex"]}, {"cell_type": "code", "execution_count": null, "id": "746dd231-3ef7-4bbe-a409-7c576fad4604", "metadata": {}, "outputs": [], "source": ["X_test_ex"]}, {"cell_type": "markdown", "id": "66173e4c-b148-4353-88c6-bb29f2a672aa", "metadata": {}, "source": ["### Dropping the `Height` and `Weight` Columns\n", "As with the cross-validation and training stages, we create a variant of the `X` sets without the height and weight."]}, {"cell_type": "code", "execution_count": null, "id": "fa323421-f1fa-486f-b14c-d8aa8bdfbb36", "metadata": {}, "outputs": [], "source": ["X_train_ex_no_hw = X_train_ex.drop([\"Height\", \"Weight\"], axis=1)\n", "X_train_ex_no_hw"]}, {"cell_type": "code", "execution_count": null, "id": "8875cd29-5227-40e1-9ef4-d43af640d278", "metadata": {}, "outputs": [], "source": ["X_test_ex_no_hw = X_test_ex.drop([\"Height\", \"Weight\"], axis=1)\n", "X_test_ex_no_hw"]}, {"cell_type": "markdown", "id": "94eab688-9964-46b0-83b7-7208a4d7069d", "metadata": {}, "source": ["## Converting Between Encodings\n", "We need a function to transform the format of the data back into a format that our models understand. For example, our model trained with the weight column understands data in the following format:"]}, {"cell_type": "code", "execution_count": null, "id": "6d09b004-80b1-4e06-9edc-97b864236729", "metadata": {}, "outputs": [], "source": ["X_test.iloc[[0]]"]}, {"cell_type": "markdown", "id": "e73e15c4-3469-467c-b0d9-fb0a4fd901d9", "metadata": {}, "source": ["But the data the explainer sees is in the following format:"]}, {"cell_type": "code", "execution_count": null, "id": "e2ef68f9-73cc-42bd-9f17-1f4633927a3f", "metadata": {}, "outputs": [], "source": ["X_test_ex.iloc[[0]]"]}, {"cell_type": "markdown", "id": "10336af9-3d51-4432-8e3d-f5dc8ba29312", "metadata": {}, "source": ["More specifically, it sees the data as an `ndarray`:"]}, {"cell_type": "code", "execution_count": null, "id": "5f20120d-1965-4166-9f99-fe48a552581d", "metadata": {}, "outputs": [], "source": ["X_test_ex.iloc[[0]].to_numpy()"]}, {"cell_type": "markdown", "id": "452109dd-6353-4de3-85b7-f82752870d84", "metadata": {}, "source": ["We define a function to convert the explainer's format to our model's format:"]}, {"cell_type": "code", "execution_count": null, "id": "580af98e-dace-466c-9c04-7d1729814537", "metadata": {}, "outputs": [], "source": ["categorical_feature_cols = [X_test_ex.columns.get_loc(feature) for feature in label_encoders.keys()]\n", "numerical_features = [\"Age\", \"Height\", \"Weight\", \"FCVC\", \"NCP\", \"CH2O\", \"FAF\", \"TUE\"]\n", "numerical_feature_cols = [X_test_ex.columns.get_loc(feature) for feature in numerical_features]\n", "\n", "height_col = X_test_ex.columns.get_loc(\"Height\")\n", "weight_col = X_test_ex.columns.get_loc(\"Weight\")\n", "\n", "def transform_for_prediction(ndarr: np.ndarray, with_hw: bool = True):\n", "    copy = ndarr.astype(object)\n", "    \n", "    if not with_hw:\n", "        # A little hacky â€” the scaler expects the height and weight columns to be there, but they aren't,\n", "        # so we add fake height and weight columns temporarily, then remove them after scaling.\n", "        fake_col_1 = min(height_col, weight_col)\n", "        fake_col_2 = max(height_col, weight_col)\n", "        \n", "        copy_with_fake = np.zeros((copy.shape[0], copy.shape[1] + 2))\n", "        copy_with_fake[:, :fake_col_1] = copy[:, :fake_col_1]\n", "        copy_with_fake[:, fake_col_1 + 1:fake_col_2] = copy[:, fake_col_1:fake_col_2]\n", "        copy_with_fake[:, fake_col_2 + 1:] = copy[:, fake_col_2 - 1:]\n", "        \n", "        copy = copy_with_fake.astype(object)\n", "    \n", "    # Scale numerical features\n", "    copy[:, numerical_feature_cols] = scaler.transform(copy[:, numerical_feature_cols])\n", "\n", "    # Undo the label encoding. Remember that all our categorical features happen to be nominal,\n", "    # so we used one-hot encoding instead.\n", "    for (feature, label_encoder), col in zip(label_encoders.items(), categorical_feature_cols):\n", "        copy[:, col] = label_encoder.inverse_transform(copy[:, col].astype(\"int\"))\n", "\n", "    # One-hot encode the nominal features.\n", "    oh_encoded = nominal_ohe.transform(copy[:, categorical_feature_cols])\n", "\n", "    if with_hw:\n", "        # Drop old columns that have been one-hot encoded.\n", "        copy = np.delete(copy, categorical_feature_cols, axis=1)\n", "    else:\n", "        # Drop old columns that have been one-hot encoded and the fake columns.\n", "        copy = np.delete(copy, categorical_feature_cols + [height_col, weight_col], axis=1)\n", "\n", "    # Concatenate with the new one-hot encoded columns.\n", "    copy = np.concatenate((copy, oh_encoded), axis=1)\n", "\n", "    return copy.astype(\"float\")"]}, {"cell_type": "markdown", "id": "9e95e5b1-b436-44a6-a90b-a7a78568a1d0", "metadata": {}, "source": ["Just to make sure the function works, we perform a sanity check. The two arrays below should be exactly the same:"]}, {"cell_type": "code", "execution_count": null, "id": "0e369ca4-65d9-4107-bd91-cd361c8a9489", "metadata": {}, "outputs": [], "source": ["X_test.iloc[[0]].to_numpy()"]}, {"cell_type": "code", "execution_count": null, "id": "e97712d7-c792-47e3-a6c5-a7e66f8741ed", "metadata": {}, "outputs": [], "source": ["transform_for_prediction(X_test_ex.iloc[[0]].to_numpy())"]}, {"cell_type": "markdown", "id": "289c9828-95c6-4fd4-861b-09cd285988f1", "metadata": {}, "source": ["And so should the following two:"]}, {"cell_type": "code", "execution_count": null, "id": "1abd7bb7-39e0-48fe-b2d8-e6c3a05f80e8", "metadata": {}, "outputs": [], "source": ["X_test.drop([\"Height\", \"Weight\"], axis=1).iloc[[0]].to_numpy()"]}, {"cell_type": "code", "execution_count": null, "id": "2a1e5f8d-6f81-4746-919c-b1afc5c50d35", "metadata": {}, "outputs": [], "source": ["transform_for_prediction(X_test_ex_no_hw.iloc[[0]].to_numpy(), with_hw=False)"]}, {"cell_type": "markdown", "id": "f2d0a97d-30be-4416-96cc-503ec2b7b480", "metadata": {}, "source": ["## Creating the Explainers\n", "The data is now in a suitable format. We now create two explainers: one for the `X` data with the weight column, and one for the `X` data without the weight column."]}, {"cell_type": "code", "execution_count": null, "id": "6a715504-9a07-4e79-8aa5-bfb51b57f817", "metadata": {}, "outputs": [], "source": ["categorical_names = {X_test_ex.columns.get_loc(feature): list(le.classes_) for feature, le in label_encoders.items()}"]}, {"cell_type": "code", "execution_count": null, "id": "3c5069ba-241d-4e5f-ab70-d70c6a371279", "metadata": {}, "outputs": [], "source": ["# Convert \"No\" to \"Non-obese\" and \"Yes\" to \"Obese\" just for better clarity in the generated diagrams.\n", "class_names = [\"Non-obese\" if cls == \"No\" else \"Obese\" for cls in target_le.classes_]"]}, {"cell_type": "code", "execution_count": null, "id": "3502ad0d-ec69-44a0-b0f8-ab56a17d1b22", "metadata": {}, "outputs": [], "source": ["explainer = lime.lime_tabular.LimeTabularExplainer(\n", "    training_data=X_train_ex.to_numpy(),\n", "    feature_names=X_train_ex.columns,\n", "    class_names=class_names,\n", "    categorical_features=categorical_feature_cols,\n", "    categorical_names=categorical_names,\n", "    mode='classification',\n", "    random_state=RAND_STATE\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "4feebe11-4f83-47cc-96e7-4ff53b28f1b9", "metadata": {}, "outputs": [], "source": ["categorical_feature_cols_no_hw = [X_test_ex.drop([\"Height\", \"Weight\"], axis=1).columns.get_loc(feature) for feature in label_encoders.keys()]\n", "categorical_names_no_hw = {X_test_ex.drop([\"Height\", \"Weight\"], axis=1).columns.get_loc(feature): list(le.classes_) for feature, le in label_encoders.items()}"]}, {"cell_type": "code", "execution_count": null, "id": "71378676-7fad-4b4f-8d81-1abcac850250", "metadata": {}, "outputs": [], "source": ["explainer_no_hw = lime.lime_tabular.LimeTabularExplainer(\n", "    training_data=X_train_ex_no_hw.to_numpy(),\n", "    feature_names=X_train_ex_no_hw.columns,\n", "    class_names=class_names,\n", "    categorical_features=categorical_feature_cols_no_hw,\n", "    categorical_names=categorical_names_no_hw,\n", "    mode='classification',\n", "    random_state=RAND_STATE\n", ")"]}, {"cell_type": "markdown", "id": "feff5731-2019-4486-bfa6-3f8f3d187c98", "metadata": {}, "source": ["## Explaining Instances\n", "We can finally use the explainers to explain some instances. We randomly choose a non-obese instance and an obese instance:"]}, {"cell_type": "code", "execution_count": null, "id": "3699d20b-d567-4956-a952-ea1a12846f77", "metadata": {}, "outputs": [], "source": ["X_test_ex_with_oblevel = pd.concat((X_test_ex, y_test), axis=1)\n", "instances = X_test_ex_with_oblevel.groupby(\"Obese\").sample(random_state=RAND_STATE).drop(\"Obese\", axis=1)\n", "instances"]}, {"cell_type": "code", "execution_count": null, "id": "bbc4a5bc-7025-433a-afd0-79295b9a65dd", "metadata": {}, "outputs": [], "source": ["def explain_instances(explainer, instances, model, with_hw=True):\n", "    # Convert the instance to an ndarray.\n", "    instances_arr = instances.to_numpy()\n", "    for instance in instances_arr:\n", "        # The code below will generate warnings about how our encoders were fitted to labelled data,\n", "        # but now we're running them on unlabelled data.\n", "        # These warnings are safe to ignore.\n", "        with warnings.catch_warnings():\n", "            warnings.simplefilter('ignore')\n", "            explanation = explainer.explain_instance(\n", "                data_row=instance,\n", "                predict_fn=lambda instances: model.predict_proba(transform_for_prediction(instances, with_hw=with_hw)),\n", "                top_labels=1,\n", "            )\n", "            explanation.show_in_notebook(show_table=True, show_all=False)"]}, {"cell_type": "markdown", "id": "2bcc2665-247a-43fc-b176-7d1cad8c7810", "metadata": {}, "source": ["### With Height and Weight"]}, {"cell_type": "markdown", "id": "9efb3049-cb89-427e-81d0-79b8b22ba59c", "metadata": {}, "source": ["Now, we explain the model's predictions for the instances using the explainer with height and weight first:"]}, {"cell_type": "code", "execution_count": null, "id": "ea999240-413c-44a0-b3ba-82ccc7580d0b", "metadata": {}, "outputs": [], "source": ["explain_instances(explainer, instances, rand_forest)"]}, {"cell_type": "markdown", "id": "c80ca82e-bc5e-4956-b74b-5f40d4bfe830", "metadata": {}, "source": ["### Without Height and Weight\n", "We now explain the instances with the model trained without the height and weight:"]}, {"cell_type": "code", "execution_count": null, "id": "0141c549-9326-48cc-adb4-a924f5ac9444", "metadata": {}, "outputs": [], "source": ["instances_no_hw = instances.drop([\"Height\", \"Weight\"], axis=1)\n", "explain_instances(explainer_no_hw, instances_no_hw, rand_forest_no_hw, with_hw=False)"]}, {"cell_type": "markdown", "id": "96079a6c-a653-4f9d-a52b-ca5e3ec75664", "metadata": {}, "source": ["## Explaining a Representative Set of Instances\n", "To gain global insights into our models' predictions, we can use SP-LIME (submodular-pick LIME), which picks out representative samples from our dataset to explain.\n", "We create SP explainers (**the cells below will take some time to run; please be patient**):"]}, {"cell_type": "code", "execution_count": null, "id": "5cd82483-f5ff-4613-abfc-c8c666cdcf68", "metadata": {}, "outputs": [], "source": ["def create_sp_explainer(explainer, model, X, with_hw=True):\n", "    # The code below will generate tons of warnings about how our encoders were fitted to labelled data, but now we're running them on unlabelled data.\n", "    # These warnings are safe to ignore.\n", "    with warnings.catch_warnings():\n", "        warnings.simplefilter('ignore')\n", "        return lime.submodular_pick.SubmodularPick(\n", "            explainer,\n", "            X.to_numpy(),\n", "            predict_fn=lambda instances: model.predict_proba(transform_for_prediction(instances, with_hw=with_hw)),\n", "            sample_size=200,\n", "            num_exps_desired=4,\n", "            num_features=len(X.columns)\n", "        )"]}, {"cell_type": "code", "execution_count": null, "id": "258c6bc6-1d85-4b68-9621-571762abec8a", "metadata": {}, "outputs": [], "source": ["sp_explainer = create_sp_explainer(explainer, rand_forest, X_test_ex)"]}, {"cell_type": "code", "execution_count": null, "id": "277e6623-ad54-4d7f-9d0d-fb04a7b1774f", "metadata": {}, "outputs": [], "source": ["sp_explainer_no_hw = create_sp_explainer(explainer_no_hw, rand_forest_no_hw, X_test_ex_no_hw, with_hw=False)"]}, {"cell_type": "markdown", "id": "2c1c20a6-59b6-4d8e-996d-a8587c9c3983", "metadata": {}, "source": ["### With Weight\n", "We can now show the explanations for these representative samples:"]}, {"cell_type": "code", "execution_count": null, "id": "98512bbd-2edc-4760-827b-cbc8111c3ab7", "metadata": {}, "outputs": [], "source": ["for exp in sp_explainer.sp_explanations:\n", "    exp.as_pyplot_figure(label=exp.available_labels()[0])"]}, {"cell_type": "markdown", "id": "0691b001-86d5-412e-90f0-7c221b90bd31", "metadata": {}, "source": ["### Without Weight"]}, {"cell_type": "code", "execution_count": null, "id": "712940cb-22bf-41a1-b164-72ff46b25620", "metadata": {}, "outputs": [], "source": ["for exp in sp_explainer_no_hw.sp_explanations:\n", "    exp.as_pyplot_figure(label=exp.available_labels()[0])"]}], "metadata": {"language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 5}
